<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://AnthonyHongXiao.github.io/images/favicon.png><title>High-Dimensional Probability and Statistics | Energy = Milk · Coffee^2</title><meta name=title content="High-Dimensional Probability and Statistics"><meta name=description content="This file is for the notes and exercises for the book Roman Vershynin&rsquo;s High-Dimensional Probability, one of the references used in Prof Lunde&rsquo;s Math5440. The other references are Ramon van Handel&rsquo;s Probability in High Dimension and Martin J.Wainwright&rsquo;s High-Dimensional Statistics.
Errata and update of HDP: errata, update.
Exercises solns for HDP: soln.
Exercises solns for HDS: soln.
0. Appetizer Key Takeaways convex combination $\sum_{i=1}^m \lambda_i z_i \quad$ where $\quad \lambda_i \geq 0 \quad$ and $\quad \sum_{i=1}^m \lambda_i=1$."><meta name=keywords content="probability,"><meta property="og:title" content="High-Dimensional Probability and Statistics"><meta property="og:description" content="This file is for the notes and exercises for the book Roman Vershynin&rsquo;s High-Dimensional Probability, one of the references used in Prof Lunde&rsquo;s Math5440. The other references are Ramon van Handel&rsquo;s Probability in High Dimension and Martin J.Wainwright&rsquo;s High-Dimensional Statistics.
Errata and update of HDP: errata, update.
Exercises solns for HDP: soln.
Exercises solns for HDS: soln.
0. Appetizer Key Takeaways convex combination $\sum_{i=1}^m \lambda_i z_i \quad$ where $\quad \lambda_i \geq 0 \quad$ and $\quad \sum_{i=1}^m \lambda_i=1$."><meta property="og:type" content="article"><meta property="og:url" content="https://AnthonyHongXiao.github.io/reading/high_dim_prob/"><meta property="og:image" content="https://AnthonyHongXiao.github.io/images/share.png"><meta property="article:section" content="reading"><meta property="article:published_time" content="2024-01-14T12:01:56+08:00"><meta property="article:modified_time" content="2024-01-14T12:01:56+08:00"><meta property="og:site_name" content="Hugo ʕ•ᴥ•ʔ Bear"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://AnthonyHongXiao.github.io/images/share.png"><meta name=twitter:title content="High-Dimensional Probability and Statistics"><meta name=twitter:description content="This file is for the notes and exercises for the book Roman Vershynin&rsquo;s High-Dimensional Probability, one of the references used in Prof Lunde&rsquo;s Math5440. The other references are Ramon van Handel&rsquo;s Probability in High Dimension and Martin J.Wainwright&rsquo;s High-Dimensional Statistics.
Errata and update of HDP: errata, update.
Exercises solns for HDP: soln.
Exercises solns for HDS: soln.
0. Appetizer Key Takeaways convex combination $\sum_{i=1}^m \lambda_i z_i \quad$ where $\quad \lambda_i \geq 0 \quad$ and $\quad \sum_{i=1}^m \lambda_i=1$."><meta itemprop=name content="High-Dimensional Probability and Statistics"><meta itemprop=description content="This file is for the notes and exercises for the book Roman Vershynin&rsquo;s High-Dimensional Probability, one of the references used in Prof Lunde&rsquo;s Math5440. The other references are Ramon van Handel&rsquo;s Probability in High Dimension and Martin J.Wainwright&rsquo;s High-Dimensional Statistics.
Errata and update of HDP: errata, update.
Exercises solns for HDP: soln.
Exercises solns for HDS: soln.
0. Appetizer Key Takeaways convex combination $\sum_{i=1}^m \lambda_i z_i \quad$ where $\quad \lambda_i \geq 0 \quad$ and $\quad \sum_{i=1}^m \lambda_i=1$."><meta itemprop=datePublished content="2024-01-14T12:01:56+08:00"><meta itemprop=dateModified content="2024-01-14T12:01:56+08:00"><meta itemprop=wordCount content="3049"><meta itemprop=image content="https://AnthonyHongXiao.github.io/images/share.png"><meta itemprop=keywords content="probability,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:1000px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444;font-size:14}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#e0ffff}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}.imagecenter{text-align:center}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script></head><body><header><a href=/ class=title><h2>Energy = Milk · Coffee^2</h2></a><nav><a href=/>Home</a>
<a href=/course/>Courses</a>
<a href=/blog>Blog</a>
<a href=/reading>Reading</a></nav></header><main><content><p>This file is for the notes and exercises for the book Roman Vershynin&rsquo;s <a href=/pdfs/HDP-book.pdf>High-Dimensional Probability</a>, one of the references used in Prof Lunde&rsquo;s Math5440. The other references are Ramon van Handel&rsquo;s <a href=%5Cpdfs%5CPHD-book.pdf>Probability in High Dimension</a> and Martin J.Wainwright&rsquo;s <a href=/pdfs/HDS-book.pdf>High-Dimensional Statistics</a>.</p><p>Errata and update of HDP: <a href=/pdfs/Vershynin-Errata-2020.pdf>errata</a>, <a href=/pdfs/Vershynin-Updates-2020.pdf>update</a>.</p><p>Exercises solns for HDP: <a href=https://zhuanlan.zhihu.com/p/338822722>soln</a>.</p><p>Exercises solns for HDS: <a href=https://high-dimensional-statistics.github.io/>soln</a>.</p><h1 id=0-appetizer>0. Appetizer</h1><h2 id=key-takeaways>Key Takeaways</h2><ol><li><p><strong>convex combination</strong> $\sum_{i=1}^m \lambda_i z_i \quad$ where $\quad \lambda_i \geq 0 \quad$ and $\quad \sum_{i=1}^m \lambda_i=1$.</p></li><li><p><strong>convex hull of a set $T$</strong>, $\mathrm{conv}(T)$ is the set of all convex combinations of $z_1, \cdots, z_m \in T$ for $m \in \mathbb{N}$.</p></li><li><p><strong>Theorem 0.0.1 (Caratheodory&rsquo;s Theorem)</strong>: Every point in the convex hull of a set $T\subseteq \mathbb{R}^n$ can be expressed as a convex combination of at most $n+1$ points from $T$.</p></li><li><p><strong>Theorem 0.0.2 (Approximate form of Caratheodory&rsquo;s Theorem)</strong>: Consider a set $T\subseteq \mathbb{R}^n$ whose diameter is bounded by $1$. Then, for every point $x\in \mathrm{conv}(T)$ and every integer $k$, one can find points $x_1,\cdots,x_k\in T$ such that
$$
\left\Vert x-\frac{1}{k}\sum^{k}_{j=1}x_j\right\Vert^2_2\le \frac{1}{\sqrt{k}}
$$</p></li><li><p><strong>Corollary 0.0.4 (Covering polytopes by balls)</strong>: Let $P$ be a polytope in $\mathbb{R}^n$ with $N$ vertices and whose diameter is bounded by $1$. Then $P$ can be covered by at most $N^{\lceil{1}/{\varepsilon^2}\rceil}$ Euclidean balls of radii $\varepsilon>0$.</p></li></ol><h2 id=exercises>Exercises</h2><h3 id=exercise-003>Exercise 0.0.3</h3><p>Check the following variance identities, which we used in the proof of Theorem 0.0.2:</p><p><strong>(a)</strong> Let $Z_1, \ldots, Z_k$ be independent mean-zero random vectors in $\mathbb{R}^n$. Show that
$$
\mathbb{E}\left\Vert\sum_{j=1}^k Z_j\right\Vert_2^2=\sum_{j=1}^k \mathbb{E}\left\Vert Z_j\right\Vert_2^2 .
$$</p><p><strong>(b)</strong> Let $Z$ be a random vector in $\mathbb{R}^n$. Show that
$$
\mathbb{E}\Vert Z-\mathbb{E} Z\Vert_2^2=\mathbb{E}\Vert Z\Vert_2^2-\Vert\mathbb{E} Z\Vert_2^2
$$
<em>Soln</em>: routine (write each random vector as a tuple of random variables and remember to use independency and zero-mean).</p><h3 id=exercise-005>Exercise 0.0.5</h3><p>(The sum of binomial coefficients). Prove the inequalities
$$
\left(\frac{n}{m}\right)^m \leq\left(\begin{array}{c}
n \newline
m
\end{array}\right) \leq \sum_{k=0}^m\left(\begin{array}{l}
n \newline
k
\end{array}\right) \leq\left(\frac{e n}{m}\right)^m
$$
for all integers $m \in[1, n]$.
Hint: To prove the upper bound, multiply both sides by the quantity $(m / n)^m$, replace this quantity by $(m / n)^k$ in the left side, and use the Binomial Theorem.</p><p><em>Soln</em>: For the first inequality:
$$
\begin{aligned}
\mathrm{RHS}&=\frac{n!}{(n-m)!m!}=\frac{(n-m+1)\times\cdots\times n}{1\times \cdots\times m}\newline&=\left(\frac{n-m+1}{1}\right)\left(\frac{n-m+2}{2}\right)\cdots\left(\frac{n-1}{m-1}\right)\left(\frac{n}{m}\right)\newline
&\ge \left(\frac{n}{m}\right)\left(\frac{n}{m}\right)\cdots\left(\frac{n}{m}\right)\left(\frac{n}{m}\right)\newline
&=\mathrm{LHS}
\end{aligned}
$$
The second inequality is trivial as the right hand side contains $n\choose m$.</p><h4 id=lemma-lim-_n-rightarrow-inftyleft1fracxnrightnex>Lemma $\lim _{n \rightarrow \infty}\left(1+\frac{x}{n}\right)^n=e^x$</h4><p><em>proof</em>:
$$
\begin{aligned}
\lim_{n\to\infty}\left(1+\frac{x}{n}\right)^n&=\lim_{n\to\infty}e^{n \log \left(1+\frac{x}{n}\right)}\newline
&=e^{\lim_{n\to\infty}\frac{\log(1+x/n)}{1/n}}\newline
&=e^{\lim_{n\to\infty}\frac{(-x/n^2)(1/(1+x/n))}{-1/n^2}}\newline
&=e^{\lim_{n\to\infty}\frac{nx}{(x+n)}}\newline
&=e^{\lim_{n\to\infty}\frac{x}{1}}\newline
&=e^x
\end{aligned}
$$
The thrid inequality is then obtained using above lemma:
$$
\sum_{k=0}^m\left(\begin{array}{c}n \newline k\end{array}\right)\left(\frac{m}{n}\right)^m \leq \sum_{k=0}^n\left(\begin{array}{c}n \newline k\end{array}\right)\left(\frac{m}{n}\right)^k=\left(1+\frac{m}{n}\right)^n \leq e^m
$$</p><h3 id=exercise-006>Exercise 0.0.6</h3><p>(Improved covering). Check that in Corollary 0.0.4,
$$
\left(C+C \varepsilon^2 N\right)^{\left\lceil 1 / \varepsilon^2\right\rceil}
$$
suffice. Here $C$ is a suitable absolute constant. (Note that this bound is slightly stronger than $N^{\left\lceil 1 / \varepsilon^2\right\rceil}$ for small $\varepsilon$.)
Hint: The number of ways to choose $k$ elements from an $N$-element set with repetitions is $\left(\begin{array}{c}N+k-1 \newline k\end{array}\right)$. Simplify using Exercise 0.0.5.</p><p><em>soln</em>:</p><p>The number of ways to choose $k$ out of $N$ objects with repetition is
$$
\left(\begin{array}{c}
N+k-1 \newline
k
\end{array}\right) .
$$</p><p>Then, use inequalities in Exercise 0.0.5 to obtain
$$
\begin{aligned}
\left(\begin{array}{c}
N+k-1 \newline
k
\end{array}\right) & \leq\left[\frac{e(N+k-1)}{k}\right]^k=\left[\frac{e(k-1)}{k}+\frac{e N}{k}\right]^k \newline
& =\left[\frac{e(k-1)}{k}+\frac{e}{k \epsilon^2} \epsilon^2 N\right]^k \leq\left[C_\epsilon\left(1+\epsilon^2 N\right)\right]^{\left\lceil 1 / \epsilon^2\right\rceil}
\end{aligned}
$$
where $C_\epsilon=\frac{e}{\left[1 / \epsilon^2\right] \epsilon^2}$.</p><h1 id=1-preliminaries-on-random-variables>1. Preliminaries on Random Variables</h1><h2 id=key-takeaways-1>Key Takeaways</h2><ol><li><p><strong>Random Variables</strong>, <strong>expectation</strong> $\mathbb{E}(X)$, <strong>variance</strong> $\mathbb{E}(X-\mathbb{E}(X))^2$, <strong>moment generating function</strong> $M_X(t)=\mathbb{E}(e^{tX})$, <strong>p-th moment</strong> $\mathbb{E}(X^p)$, <strong>p-th absolute moment</strong> $\mathbb{E}(|X|^p)$, and <strong>$L^P$ norm</strong> and <strong>essential norm</strong> of a random variable
$$
\begin{aligned}
\Vert X\Vert_{L^p}&=(\mathbb{E}|X|^p)^{1/p}, p\in (0,\infty)\
\Vert X\Vert_{L^{\infty}}&=\mathrm{ess; sup}|X|=\inf{M|;|f|\le M\text{ for }\mu-\text{a.e. }x\in X}
\end{aligned}
$$</p></li><li><p>For fixed $p$ and a given probability space $(\Omega, \Sigma, \mathbb{P})$, the classical vector space $L^p=L^p(\Omega, \Sigma, \mathbb{P})$ consists of all random variables $X$ on $\Omega$ with finite $L^p$ norm, that is
$$
L^p=\set{X:|X|_{L^p}&lt;\infty}.
$$</p><p>If $p \in[1, \infty]$, the quantity $|X|<em>{L^p}$ is a norm and $L^p$ is a <strong>Banach space</strong>. This fact follows from <strong>Minkowski&rsquo;s inequality</strong>. For $p&lt;1$, the triangle inequality fails and $|X|</em>{L^p}$ is not a norm.</p><p>The exponent $p=2$ is special in that $L^2$ is not only a Banach space but also a <strong>Hilbert space</strong>. The inner product and the corresponding norm on $L^2$ are given by
$$
\langle X, Y\rangle_{L^2}=\mathbb{E} X Y, \quad|X|_{L^2}=\left(\mathbb{E}|X|^2\right)^{1 / 2} .
$$</p><p>Then the <strong>standard deviation</strong> of $X$ can be expressed as
$$
|X-\mathbb{E} X|_{L^2}=\sqrt{\operatorname{Var}(X)}=\sigma(X) .
$$</p><p>Similarly, we can express the <strong>covariance</strong> of random variables of $X$ and $Y$ as
$$
\operatorname{cov}(X, Y)=\mathbb{E}(X-\mathbb{E} X)(Y-\mathbb{E} Y)=\langle X-\mathbb{E} X, Y-\mathbb{E} Y\rangle_{L^2} .
$$</p></li><li><p><strong>Jensen&rsquo;s Inequality</strong>: for $\varphi:\mathbb{R}\to \mathbb{R}$​ convex, we have
$$
\varphi(\mathbb{E}X)\le \mathbb{E}\varphi(X)
$$</p></li><li><p><strong>Corollary</strong>: $\Vert X\Vert_{L^p}$ is an increasing function in $p$, i.e.
$$
\Vert X\Vert_{L^p}\le \Vert X\Vert_{L^q}\quad \text{for any }0\le p\le q=\infty
$$</p></li><li><p><strong>Minkowski&rsquo;s inequality</strong>: for any $p\in [1,\infty]$ and any random variables $X,Y\in L^p$​, we have
$$
\Vert X+Y\Vert_{L^p}\le \Vert X\Vert_{L^p}+\Vert Y\Vert_{L^p}
$$</p></li><li><p><strong>Cauchy-Schwarz inequality</strong>: for any random variables $X,Y\in L^2$, we have
$$
|\mathbb{E}XY|\le \Vert X\Vert_{L^2}\Vert Y\Vert_{L^2}
$$</p></li><li><p><strong>Holder inequality</strong>: suppose $p,q\in (1,\infty)$ such that $1/p+1/q=1$, i.e., they are <strong>conjugate exponents</strong>. The random variables $X\in L^p$ and $Y\in L^q$​ satisfy
$$
|\mathbb{E}XY|\le \Vert X\Vert_{L^p}\Vert Y\Vert_{L^q}
$$
The inequality also holds for the pair $p=1,q=\infty$.</p></li><li><p>Definition of <strong>distribution</strong>, <strong>propability density function (pdf)</strong>, and <strong>cumulative distribution function (cdf)</strong>.</p></li><li><p><strong>Lemma 1.2.1 (Integral identity)</strong>. Let $X$ be a non-negative random variable. Then
$$
\mathbb{E} X=\int_0^{\infty} \mathbb{P}{X>t} d t .
$$</p><p>The two sides of this identity are either finite or infinite simultaneously.</p></li><li><p><strong>Exercise 1.2.2 (Generalization of integral identity)</strong>. for any random variable $X$ (not necessarily non-negative):
$$
\mathbb{E} X=\int_0^{\infty} \mathbb{P}{X>t} d t-\int_{-\infty}^0 \mathbb{P}{X&lt;t} d t
$$</p></li><li><p><strong>Exercise 1.2.3 ( $p$-moments via tails)</strong>. Let $X$ be a random variable and $p \in(0, \infty)$. Show that
$$
\mathbb{E}|X|^p=\int_0^{\infty} p t^{p-1} \mathbb{P}{|X|>t} d t
$$
whenever the right hand side is finite.</p></li><li><p><strong>Proposition 1.2.4 (Markov&rsquo;s Inequality)</strong>. For any non-negative random variable $X$ and $t>0$, we have
$$
\mathbb{P}{X \geq t} \leq \frac{\mathbb{E} X}{t}
$$</p></li><li><p><strong>Corollary 1.2 .5 (Chebyshev&rsquo;s inequality)</strong>. Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. Then, for any $t>0$, we have
$$
\mathbb{P}{|X-\mu| \geq t} \leq \frac{\sigma^2}{t^2}
$$</p></li><li><p>Some formulae:</p><p><strong>Expectation and Variance of linear comb.</strong>
$$
\begin{aligned}
&\mathbb{E}(aX+b)=a\mathbb{E}(X)+b\
&\operatorname{Var}(aX+b)=a^2\operatorname{Var}(X)\
\end{aligned}
$$
<strong>Proposition (Ross (e8) p.129, p.191)</strong>: The expectation of function of random variable is given by
$$
\begin{aligned}
\mathbb{E}[g(X)]&=\sum_xg(x)p(x)\
\mathbb{E}[g(X)]&=\int_{-\infty}^{\infty}g(x)f(x)dx
\end{aligned}
$$
<strong>Proposition (Ross (e8) p.298)</strong>: if $X$ and $Y$ have a joint probability mass function $p(x,y)$, then
$$
\mathbb{E}[g(X,Y)]=\sum_y\sum_x g(x,y)p(x,y)
$$
If they have joint probability density function $f(x,y)$, then
$$
\mathbb{E}[g(X,Y)]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)f(x,y)dxdy
$$
<strong>Corollary (Ross e8 p.299)</strong>: for finite $X,Y$ and $g(X,Y)=X+Y$, we have $\mathbb E(X+Y)=\mathbb E(X)+\mathbb E(Y)$.</p><p><strong>Variance of sum of var.</strong>: if they are independent,
$$
\operatorname{Var}(X_1+\cdots+X_n)=\operatorname{Var}(X_1)+\cdots+\operatorname{Var}(X_n)
$$
if they are also identically distributed,
$$
\operatorname{Var}\left(\frac{1}{N} \sum_{i=1}^N X_i\right)=\frac{\sigma^2}{N}
$$
For more formulae, see Ross (e8) p.322 section 7.4 Covariance, Variance of Sums, and Correlations.</p></li><li><p><strong>Theorem 1.3.1 (Strong law of large numbers)</strong>. Let $X_1, X_2, \ldots$ be a sequence of i.i.d. random variables with mean $\mu$. Consider the sum
$$
S_N=X_1+\cdots +X_N .
$$</p><p>Then, as $N \rightarrow \infty$,
$$
\frac{S_N}{N} \rightarrow \mu \quad \text { almost surely. }
$$
That is, <strong>converges in probability</strong>: for any $\varepsilon>0$,
$$
\lim_{N\to \infty}\mathbb{P}\left(\left|\frac{S_N}{N}-\mu\right|\le \varepsilon\right)=1
$$</p></li><li><p><strong>Theorem 1.3.2 (Lindeberg-Lévy central limit theorem)</strong>. Let $X_1, X_2, \ldots$ be a sequence of i.i.d. random variables with mean $\mu$ and variance $\sigma^2$. Consider the sum
$$
S_N=X_1+\cdots+X_N
$$
and normalize it to obtain a random variable with zero mean and unit variance as follows:
$$
Z_N:=\frac{S_N-\mathbb{E} S_N}{\sqrt{\operatorname{Var}\left(S_N\right)}}=\frac{1}{\sigma \sqrt{N}} \sum_{i=1}^N\left(X_i-\mu\right) .
$$</p><p>Then, as $N \rightarrow \infty$,
$$
Z_N \rightarrow N(0,1) \text { in distribution. }
$$</p><p>That is, <strong>converges in distribution</strong>: for a sequence of r.v. $X_i$ with their cdf $F_{X_i}$, one has pointwise convergence of $F_{X_i}$ to cdf $F_X$ of a r.v. $X$:
$$
\forall x,\lim_{N\to \infty}F_i(x)=F(x)
$$
The convergence in distribution means that the $\mathrm{CDF}$ of the normalized sum converges pointwise to the CDF of the standard normal distribution. We can express this in terms of tails as follows. Then for every $t \in \mathbb{R}$, we have
$$
\mathbb{P}\set{Z_N \geq t} \rightarrow \mathbb{P}{g \geq t}=\frac{1}{\sqrt{2 \pi}} \int_t^{\infty} e^{-x^2 / 2} d x
$$
as $N \rightarrow \infty$, where $g \sim N(0,1)$​ is a standard normal random variable.</p></li><li><p><strong>Corollary (de Moivre-Laplace theorem)</strong>:</p><p>One remarkable special case of the central limit theorem is where $X_i$ are Bernoulli random variables with some fixed parameter $p \in(0,1)$, denoted
$$
X_i \sim \operatorname{Ber}(p) .
$$</p><p>Recall that this means that $X_i$ take values 1 and 0 with probabilities $p$ and $1-p$ respectively; also recall that $\mathbb{E} X_i=p$ and $\operatorname{Var}\left(X_i\right)=p(1-p)$. The sum
$$
S_N:=X_1+\cdots+X_N
$$
is said to have the binomial distribution $\operatorname{Binom}(N, p)$. The central limit theorem (Theorem 1.3.2) yields that as $N \rightarrow \infty$,
$$
\frac{S_N-N p}{\sqrt{N p(1-p)}} \rightarrow N(0,1) \text { in distribution. }
$$</p></li><li><p><strong>Corollary (Poisson limit theorem)</strong>:</p><p>Theorem 1.3.4 (Poisson Limit Theorem). Let $X_{N, i}, 1 \leq i \leq N$, be independent random variables $X_{N, i} \sim \operatorname{Ber}\left(p_{N, i}\right)$, and let $S_N=\sum_{i=1}^N X_{N, i}$. Assume that, as $N \rightarrow \infty$,
$$
\max_{i \leq N} p_{N, i} \rightarrow 0 \quad \text{ and } \quad \mathbb{E} S_N=\sum_{i=1}^N p_{N, i} \rightarrow \lambda&lt;\infty .
$$</p><p>Then, as $N \rightarrow \infty$,
$$
S_N \rightarrow \operatorname{Pois}(\lambda) \quad\text { in distribution. }
$$</p></li></ol><h1 id=2-concentration-of-sums-of-independent-random-variables>2. Concentration of Sums of Independent Random Variables</h1><h2 id=key-takeaways-2>Key Takeaways</h2><h3 id=21-why-concentration-inequalities>2.1 Why Concentration Inequalities?</h3><ul><li><p>An example (Question 2.1.1) on estimation of number of heads of flipping coins by Chebyshev&rsquo;s inequality and central limit theorem.</p></li><li><p>Our Math5440 gives some more examples (see Lecture 2).</p></li></ul><h3 id=22-hoeffdings-inequality>2.2 Hoeffding&rsquo;s Inequality</h3><ul><li>symmetric Bernoulli distribution is the uniform distribution for discrete variable $X$ that takes values $1$ and $-1$, i.e., $\mathbb{P}(X=1)=\mathbb{P}(X=-1)=\frac{1}{2}$.</li></ul><p><strong>Theorem 2.2.2 (Hoeffding&rsquo;s Inequality)</strong>: Let $X_1,\cdots,X_N$ be independent symmetric Bernoulli random variables, and let $a=(a_1,\cdots,a_N)\in \R^N$. Then, for any $t>0$, we have
$$
\mathbb{P}\left[\sum_{i=1}^Na_iX_i\ge t\right]\le \mathbb P\left(-\frac{t^2}{2\Vert a\Vert_2^2}\right)
$$
<strong>Remark 2.2.4 (Non-asymptotic results)</strong>: Note that CLT gives a bound of tail that works for large $N$​, while Hoeffding bound gets rid of that requirement.</p><p>Two generalizations are given:</p><p><strong>Theorem 2.2.5 (Two sided Hoeffding&rsquo;s inequality)</strong> Let $X_i$ and $a$ be the same. Then, for $t>0$,
$$
\mathbb{p}\left[\left|\sum_{i=1}^Na_iX_i\right|\ge t\right]\le 2\exp\left(-\frac{t^2}{2\Vert a\Vert_2^2}\right)
$$
<strong>Theorem 2.2.6 (Hoeffding&rsquo;s inequaity for general bounded random variables)</strong> Let $X_1,\cdots,X_N$ be independent random variables. Assume that $X_i\in [m_i,M_i]$ for every $i$. Then, for any $t>0$, we have
$$
\mathbb{P}\left[\sum_{i=1}^N(X_i-\mathbb{E}X_i)\ge t\right]\le \exp\left(-\frac{2t^2}{\sum_{i=1}^N(M_1-m_1)^2}\right)
$$</p><h3 id=23-chernoffs-inequality>2.3 Chernoff&rsquo;s Inequality</h3><p><strong>Theorem 2.3.1 (Chernoff&rsquo;s inequality (upper tail))</strong></p><p>Let $X_i$ be independent Bernoulli random variables with parameters $p_i$. Consider their sum $S_N=\sum_{i=1}^NX_i$ and denote its mean by $\mu=\mathbb{E}S_N$. Then, for $t>\mu$, we have
$$
\mathbb{E}[S_N\le t]\le e^{-\mu}\left(\frac{e\mu}{t}\right)^t
$$
<strong>Exercise 2.3.2 (Chernoff&rsquo;s inequality (lower tail))</strong></p><p>Let $X_i$ and $S_N$ be the same. When $t&lt;\mu$, we have
$$
\mathbb{E}[S_N\le t]\le e^{-\mu}\left(\frac{e\mu}{t}\right)^t
$$
<strong>Exercise 2.3.3 (Poisson tail)</strong></p><p>Let $X\sim \mathrm{Pois}(\lambda)$. When $t>\lambda$, we have
$$
\mathbb{E}[X\le t]\le e^{-\lambda}\left(\frac{e\lambda}{t}\right)^t
$$
<strong>Exercise 2.3.6 (Poisson dist. near the mean)</strong></p><p>Let $X\sim \mathrm{Pois}(\lambda)$. When $t\in (0,\lambda]$, we have
$$
\mathbb{P}[|X-\mu|\ge t]\le 2\exp\left(-\frac{ct^2}{\lambda}\right)
$$
<strong>Exercise 2.3.7 (Normal approximation to Poisson)</strong></p><p>Let $X\sim \mathrm{Pois}(\lambda)$. As $\lambda\to \infty$, we have
$$
\frac{X-\lambda}{\sqrt{\lambda}}\to N(0,1)\quad\text{in distribution}
$$</p><h3 id=24-application-degrees-of-random-graphs>2.4 Application: Degrees of Random Graphs</h3><h3 id=25-sub-gaussian-distributions>2.5 Sub-Gaussian Distributions</h3><p>If $X\sim N(0,1)$, then for $p\ge 1$,</p><ul><li>$\mathbb{P}[|X|\ge t]\le 2\exp(-t^2/2)\quad \forall t\in R$​.</li><li>$\Vert X\Vert_{L^p}=(\mathbb{E}|X|^p)^{1/p}=\sqrt{2}\left(\frac{\Gamma((1+p)/2)}{\Gamma(1/2)}\right)^{1/p}$.</li><li>$\Vert X\Vert_{L^P}=O(\sqrt{p}),\quad p\to \infty$.</li><li>Its MGF is given by $\mathbb{E}(e^{tX})=e^{t^2/2},\quad \forall t\in \R$.</li></ul><h4 id=some-facts-about-beta-and-gamma-functions>Some facts about Beta and Gamma functions</h4><p>(Rudin&rsquo;s <em>Introduction to Mathematical Analysis</em> (e3) p.192 has some of the following properties.)</p><ul><li><p>Beta Function $B(x,y)=\int_0^1 t^{x-1}(1-t)^{y-1}dt$. This indefinite integral is convergent for $x>0,y&lt;0$.</p></li><li><p>Gamma Function $\Gamma(x)=\int_0^{+\infty}t^{x-1}e^{-t}dt$. This indefinite integral is convergent for $x>0$.</p></li><li><p>property 1: for $0&lt;x&lt;\infty$, $\Gamma (x+ 1) = x\Gamma(x)$.</p></li><li><p>property 2: $\Gamma(n)=n!$ for $n=1,2,3,\cdots$.</p></li><li><p>property 3: $\log \Gamma$ is convex on $(0,\infty)$.</p></li><li><p>property 4: if $x>0$ and $y>0$, then
$$
B(x,y)=\int_0^1 t^{x-1}(1-t)^{y-1}dt=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
$$</p></li><li><p>property 5: both $\Beta$ and $\Gamma$ are continuous on their convergent regions.</p></li><li><p>property 6 (<strong>Stirling&rsquo;s Formula</strong>):
$$
\lim _{x \rightarrow \infty} \frac{\Gamma(x+1)}{(x / e)^x \sqrt{2 \pi x}}=1
$$</p></li><li><p>Property 6&rsquo; (<strong>Stirling&rsquo;s Formula</strong>):
$$
\Gamma(x)=\sqrt{\frac{2\pi}{x}}\left(\frac{x}{e}\right)^x(1+o(1/x))
$$</p></li><li><p>Property 7: for any $x\ge 1/2$, $\Gamma(x)\le 3x$.</p></li></ul><h4 id=sub-gaussian-variable>Sub-Gaussian variable</h4><p>A zero-mean random variable $X$ is <strong>sub-Gaussian</strong> with parameter $\sigma^2$, or $\operatorname{subGaussian}\left(\sigma^2\right)$, if there exists $\sigma^2 \geq 0$ such that:
$$
E\left(e^{\lambda X}\right) \leq e^{\frac{\lambda^2 \sigma^2}{2}} \text { for all } \lambda \geq 0
$$</p><p>In other words, a random variable is sub-Gaussian if its MGF is upperbounded by an MGF of some Gaussian random variable with variance $\sigma^2$. It should be noted that the parameter $\sigma^2$ is generally not the variance of a subGaussian random variable (it is the variance of a Gaussian that has a larger MGF). If this property holds, we have that:
$$
\begin{aligned}
P(X-\mu \geq t) & \leq \inf _{\lambda>0} E\left(e^{\lambda(X-\mu)}\right) e^{-\lambda t} \
& \leq \inf _{\lambda>0} \exp \left(\frac{\lambda^2 \sigma^2}{2}-\lambda t\right) \
& \left.=\exp \left(\frac{t^2}{2 \sigma^2}-\frac{t^2}{\sigma^2}\right) \text { (plugging in minimizer } \lambda=t / \sigma^2\right) \
& =\exp \left(\frac{-t^2}{2 \sigma^2}\right)
\end{aligned}
$$</p><p>Therefore, if a random variable is sub-Gaussian, we have tail decay of the form $\exp \left(-t^2 / 2 K_1^2\right)$, which is quite fast. One can show an analogous bound for the left tail. So what random variables are sub-Gaussian? It turns out that bounded random variables are sub-Gaussian.</p><p><strong>Proposition 1</strong>. Suppose $X$ is mean 0 and $a \leq X \leq b$ with probability 1. Then, $X$ is sub-Gaussian with $\sigma^2=(b-a)^2 / 4$.</p><p>In what follows, let $K_i$ be the sub-Gaussian constant associated with the $i$-th notion of sub-Gaussianity. It turns out that different notions of sub-Gaussianity are equivalent in the sense that they are related to each other by some universal constant. In other words, if $X ∼ \mathrm{subGaussian}(K_i)$, then there exists some constant $C$ (that does not depend on the distribution of $X$) such that $X ∼ \mathrm{subGaussian}(CK_j)$ for an equivalent notion of sub-Gaussianity. Another way of saying this is that for any $i,j=1,\cdots,5$, there is some absolute constant $C$ such that $K_i\le CK_j$.</p><p><strong>Proposition 2.5.2 (Sub-Gaussian Properties)</strong>: Let $X$ be a random variable. Then the following properties are equivalent.</p><p>(i) The tails of $X$ satisfy
$$
\mathbb{P}{|X| \geq t} \leq 2 \exp \left(-t^2 / K_1^2\right) \quad \text { for all } t \geq 0 .
$$
(ii) The moments of $X$ satisfy
$$
|X|_{L^p}=\left(\mathbb{E}|X|^p\right)^{1 / p} \leq K_2 \sqrt{p} \quad \text { for all } p \geq 1 .
$$
(iii) The MGF of $X^2$ satisfies
$$
\mathbb{E} \exp \left(\lambda^2 X^2\right) \leq \exp \left(K_3^2 \lambda^2\right) \quad \text { for all } \lambda \text { such that }|\lambda| \leq \frac{1}{K_3} \text {. }
$$
(iv) The MGF of $X^2$ is bounded at some point, namely
$$
\mathbb{E} \exp \left(X^2 / K_4^2\right) \leq 2 .
$$</p><p>Moreover, if $\mathbb{E} X=0$ then properties $i$-iv are also equivalent to the following one.
(v) The MGF of $X$ satisfies
$$
\mathbb{E} \exp (\lambda X) \leq \exp \left(K_5^2 \lambda^2\right) \quad \text { for all } \lambda \in \mathbb{R}
$$
<strong>Orlicz Norm</strong> and <strong>Sub-Gaussian Norm</strong>:</p><p>Suppose that $\Psi:[0, \infty) \mapsto[0, \infty)$ is a monotone increasing, convex function such that $\Psi(0)=0$ and $\lim <em>{x \rightarrow \infty} \Psi(x)=\infty$. The associated Orlicz norm of a random variable $X$ is given by
$$
|X|</em>{\Psi}=\inf \left{t>0: \mathbb E\left[\Psi\left(\frac{|X|}{t}\right)\right] \leq 1\right}
$$</p><p>It can be shown that this is indeed a norm on the space of random variables for which this quantity is finite. It can also shown that for the choice of function $\psi_2(x)=e^{x^2}-1$, the sub-Gaussian condition:
$$
\mathbb E\left(\exp \left(X^2 / K_4^2\right)\right) \leq 2
$$
is equivalent to:
$$
|X|_{\psi_2} \leq K_4
$$</p><p>We let this <strong>sub-Gaussian norm</strong> of a sub-Gaussian be specifically denoted as
$$
\Vert X\Vert_{\psi_2}=\inf {t>0:\mathbb E\exp(X^2/t^2)\le 2}
$$
Other restatements of sub-Gaussianities using the norm are given in (2.14)-(2.16):
$$
\begin{gathered}
\mathbb{P}{|X| \geq t} \leq 2 \exp \left(-c t^2 /|X|<em>{\psi_2}^2\right) \quad \text { for all } t \geq 0 \
|X|</em>{L^p} \leq C|X|<em>{\psi_2} \sqrt{p} \quad \text { for all } p \geq 1 \
\mathbb{E} \exp \left(X^2 /|X|</em>{\psi_2}^2\right) \leq 2 \
\text{if }\mathbb{E} X=0\text{ then }\mathbb{E} \exp (\lambda X) \leq \exp \left(C \lambda^2|X|_{\psi_2}^2\right) \quad\forall \lambda \in \mathbb{R}
\end{gathered}
$$
Therefore, Orlicz norms can be used to define sub-Gaussianity in a succinct way and imply other properties of these random variables. Different choices of the function $\Psi$​​ will also lead to different notions of light-tailedness.</p><p><strong>Examples and non-examples of sub-gaussian r.v.</strong></p><p>(i) <strong>Gaussian</strong></p><p>(ii) <strong>Bernoulli</strong></p><p>(iii) <strong>Bounded</strong></p><p>(iv) <strong>Poisson</strong></p><p>(v) <strong>Pareto</strong></p><p>(vi) <strong>Cauchy</strong></p><h3 id=26-general-hoeffding-and-khintchine-inequalities>2.6 General Hoeffding and Khintchine Inequalities</h3><h2 id=exercises-1>Exercises</h2><h3 id=exercise-214>Exercise 2.1.4</h3><p>Let $g\sim N(0,1)$. Show that, for all $t>1$, we have
$$
\mathbb{E}(g^2\mathbf{1}_{{g>t}})=t\frac{1}{\sqrt{2\pi}}e^{-t^2/2}+\mathbb{P}{g>t}\le \left(t+\frac{1}{t}\right)\frac{1}{\sqrt{2\pi}}e^{-t^2/2}
$$
<em>soln</em>:</p><p>The pdf of $g\sim N(0,1)$ is $p(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}$. We observe that $p&rsquo;(x)=-x\frac{1}{\sqrt{2\pi}}e^{-x^2/2}=-xp(x)$. By formula (27) (expectation of function of variable), we compute
$$
\begin{aligned}
\mathbb{E}(g^2\mathbf{1}_{{g>t}})&=\int_t^{\infty}x^2p(x)dx\
&=\int_t^{\infty}-xp&rsquo;(x)dx\
&=[-xp(x)]<em>t^{\infty}+\int_t^{\infty}p(x)dx\
&=t\frac{1}{\sqrt{2\pi}}e^{-t^2/2}+\mathbb{P}{g>t}
\end{aligned}
$$
where for the last step we notice that $\lim</em>{x\to \infty}xp(x)=0$ for $p(x)$ decreases exponentially fast to zero. This proved the equality. To show the inequality is obtained by using Proposition 2.1.2 in HDP.</p><h3 id=exercise-223>Exercise 2.2.3</h3><p>show that
$$
\cosh (x)\le e^{x^2/2},\quad \forall x\in \mathbb{R}
$$
<em>soln 1</em>:
$$
\cosh (x)=\prod_{k=1}^{\infty}\left(1+\frac{4 x^2}{\pi^2(2 k-1)^2}\right) \leq \exp \left(\sum_{k=1}^{\infty} \frac{4 x^2}{\pi^2(2 k-1)^2}\right)=\exp \left(x^2 / 2\right)
$$
<em>soln 2</em>:</p><p>This is equivalent of showing $\ln \cosh(x)\le x^2/2$. Define $f(x)=\ln (\cosh x)-x^2 / 2$ and see that $f^{\prime}(x)=\tanh (x)-x$. Let it be zero to find maximum of $f$.</p><h3 id=exercise-228>Exercise 2.2.8</h3><p>(Boosting Randomized Algorithm) Calculate the probability of correctness for majority vote where each voter has $\frac{1}{2}+\delta$ probability of being correct.</p><p><em>soln</em>:</p><p>Let each of the voter be $X_i\sim B(\frac{1}{2}+\delta)$. By Theorem 2.2.6 (Hoeffding&rsquo;s inequaity for general bounded random variables), we have
$$
\begin{aligned}
\mathbb{P}\left[\sum_{i=1}^NX_i\le \frac{N}{2}\right]&= \mathbb{P}\left[\sum_{i=1}^N(-X_i)\ge -\frac{N}{2}\right]\
&=\mathbb{P}\left[\sum_{i=1}^N((-X_i)-\mathbb{E}(-X_i))\ge -\frac{N}{2}+N\left(\frac{1}{2}+\delta\right)\right]\
&\le \exp\left(-\frac{2(N\delta)^2}{N}\right)=\exp(-2N\delta^2)
\end{aligned}
$$
When $N\ge \frac{1}{2}\delta^{-2}\ln (\varepsilon^{-1})$, we have $\exp(-2N\delta^2)\le \exp(-\ln(\varepsilon^{-1}))=\varepsilon$. The probability that the answer voted by majority is incorrect is at most $\varepsilon$.</p><h3 id=exercise-2210>Exercise 2.2.10</h3><p>Let $X_1,\cdots,X_N$ be non-negative independent random variables with continuous distributions. Assume that the densities of $X_i$ are uniformly bounded by $1$.</p><p><strong>(a)</strong> Show that the MGF of $X_i$ satisfies
$$
\mathbb{E}\exp (-tX_i)\le \frac{1}{t}\quad \forall t>0
$$
<strong>(b)</strong> Deduce that, for any $\varepsilon>0$, we have
$$
\mathbb{P}\left[\sum_{i=1}^NX_i\le \varepsilon N\right]\le (e\varepsilon)^N
$$
<em>soln</em>:</p><p><strong>(a)</strong> We note that these r.v. are non-negative, so
$$
\mathbb{E}\exp (-tX_i)\le \frac{1}{t}=\int_0^{\infty}p_i(x)e^{-tx}dx
$$
and we bound it by its absolute value and use the condition of uniform boundedness of the family,
$$
\text{RHS}\le \left|\int_0^{\infty}p_i(x)e^{-tx}dx\right|\le \int_0^{\infty}\underbrace{|p_i(x)|}<em>{\le 1}\underbrace{|e^{-tx}|}</em>{=e^{-tx}}dx\le\int_0^{\infty}e^{-tx}dx=\left[-\frac{1}{t}e^{-tx}\right]<em>0^{\infty}=\frac{1}{t}
$$
<strong>(b)</strong> We mimic the proof of Theorem 2.2.2 (Hoeffding&rsquo;s inequality), i.e., using Markov&rsquo;s inequality and the fact that MGF of sum is product of MGF for independent $X_i$:
$$
\begin{aligned}
\mathbb{P}\left[\sum</em>{i=1}^N X_i\le \varepsilon N\right]&=\mathbb{P}\left[\exp\sum_{i=1}^N(-tX_i)\ge \exp(-t\varepsilon N)\right]\
&\le \frac{\mathbb{E}\left(\exp \sum_{i=1}^N(-tX_i)\right)}{\exp(-t\varepsilon N)}\
&=\frac{\prod_{i=1}^N\mathbb{E}\left(\exp (-tX_i)\right)}{\exp(-t\varepsilon N)}\
&\le e^{t\varepsilon N}\prod_{i=1}^N\frac{1}{t}=e^{t\varepsilon N}/t^N
\end{aligned}
$$
Letting $t=1/\varepsilon$ to maximize it to get $(e\varepsilon)^N$​​​.</p><h3 id=exercise-259>Exercise 2.5.9</h3><p>We did this when listing examples and non-examples of sub-Gaussians above.</p></content><p><a href=https://AnthonyHongXiao.github.io/blog/probability/>#probability</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>